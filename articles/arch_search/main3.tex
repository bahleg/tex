\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{cmap}
\usepackage[T2A]{fontenc}



\usepackage{amssymb,amsfonts,amsthm,amsmath,mathtext,cite,enumerate,float}
\usepackage[russian, english]{babel}
\usepackage{graphicx}
\usepackage{tabularx}

\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{a4wide}
\usepackage{cite}

\usepackage{caption}
\usepackage{subcaption}
\usepackage{multicol}
\usepackage{multirow}
\sloppy
%\renewcommand{\headrulewidth}{0pt}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\begin{document}
\section{Аннотация}
В работе рассматривается задача выбора структуры модели глубокого обучения.
Модель --- это вычислительный вероятностный граф, т.е. граф, в котором ребрами выступают нелинейные функции, а вершинами --- результаты действия функцией на выборку.  Каждому ребру поставлено в соответствие множество нелинейных функций, такое что линейная комбинация этих функций определяет дифференцируемую функцию заданной сигнатуры. Структурой модели назовем веса линейной комбинации этих функций. 

Для нахождения оптимальной структуры предлагается ввести вероятностную интерпретацию модели, т.е. предположения о распределениях параметров и структуры модели. 
Проводится градиентная оптимизация параметров и гиперпараметров модели на основе байесовского вариационного вывода. Решается двухуровневая задача оптимизации: на первом уровне проводится оптимизация нижней оценки правдоподобия модели по вариационным параметрам модели. На втором уровне проводится оптимизация гиперпараметров модели. В качестве оптимизируемой функции для гиперпараметров модели предлагается обобщенная функция правдоподобия. Показано, что данная функция позволяет проводить оптимизацию  несколькими алгоритмами: последовательным добавлением и удалением параметров, полным перебором, а также максимизацией нижней оценки правдоподобия модели.

Проводится сравнение с эвристическими алгоритмами выбора структуры модели. Вычислительный эксперимент проводится на синтетических данных и выборке рукописных цифр MNIST.

\textbf{Цель работы:} предложить метод выбора модели субоптимальной сложности, позволяющий проводить выбор модели в нескольких режимах (ELBO, AddDel, полный перебор, оптимизация без регуляризации и с регуляризацией).



\section{Постановка задачи}
Задана выборка \begin{equation}\label{eq:dataset}\mathfrak{D} = \{(\mathbf{x}_i,y_i)\}, i = 1,\dots,m,\end{equation} состоящая из множества пар <<объект-метка>> $$\mathbf{x}_i \in \mathbf{X} \subset \mathbb{R}^n, \quad {y}_i \in \mathbf{y} \subset \mathbb{Y}.$$ Метка ${y}$  объекта $\mathbf{x}$ принадлежит либо множеству: ${y} \in \mathbb{Y} = \{1, \dots, Z\}$ в случае задачи классификации, где $Z$ --- число классов, либо некоторому подмножеству вещественных чисел ${y} \in \mathbb{Y}  \subseteq \mathbb{R}$ в случае задачи регрессии. Далее будем полагать, что объекты $\mathbf{x}$ являются реализацией некоторой случайно величины и порождены независимо.

Определим семейство моделей глубокого обучения для дальнейшего выбора оптимальной модели. 
Будем рассматривать семейство моделей как граф $V,E$. Каждому ребру $(i,j) \in E$ сопоставим множество функций $\mathbf{g}^{i,j}$ мощности $K^{i,j}$. Вершины $V$ --- промежуточные промежуточные представления выборки под действием данных функций. 

Перейдем к формальному определению модели.
Пусть задан граф $V,E$. Пусть для каждого ребра $(i,j) \in E$ определено множество функций $\mathbf{g}^{i,j}.$ Граф $V, E$ называется семейством моделей, если функция, задаваемая рекурсивно как 
\[
    f_j(\mathbf{x}) = \sum_{k \in \text{Adj}(v_j)} <\gamma^{j,k}, \mathbf{g}^{j,k}>\left(\mathbf{f}_k(\mathbf{x})]\right), \quad \mathbf{f}_0(\mathbf{x}) = \mathbf{x}
\]
является дифференцируемой по параметрам функцией из $\mathbb{R}^n$ во множество $\mathbb{Y}$ при любых значениях векторов $\gamma^{j,k}$.

Параметрами модели $\mathbf{W}$ будем называть конкатенацию всех параметров подмоделей $\mathbf{f}_j$.
Структурой модели $\boldsymbol{\Gamma}$ будем называть конкатенацию всех структурных параметров $\gamma^{j,k}$.
Моделью будем называть совокупность параметров $\mathbf{W}$ и гиперпараметров $\mathbf{W}$.

Пусть все векторы $\gamma^{i,j}$ являются нормированными и положительными.
Пусть для каждого структурного параметра $\gamma^{j,k} \in \boldsymbol{\Gamma}$ определено априорное Gumbel-softmax распределение $p(\gamma^{j,k}|\mathbf{m}^{j,k}, c_{\text{temp}})$ с параметром средних $\mathbf{m}$ и температурой $c_{\text{temp}}$. 
 
Пусть для структуры модели определено априорное распределение $p(\boldsymbol{\Gamma}|\mathbf{m})$, где $\mathbf{m}$ --- некоторое распределение. \textit{(нужно ли определять его явно здесь?)}
Пусть для каждого параметра $w \in \mathbf{W}$ определено множество $\mathcal{S}(w)$ структурных параметров $\gamma$, соответсвтующих базовым функциям $\mathbf{g}$, для которых определен этот параметр:
\[
    {w} \sim \mathcal{N}(\mathbf{0}, {a}^{-1} \cdot (\sum_{\gamma \in \mathcal{S}(w)}\gamma),
\]
где $a$ --- гиперпараметр, входящий в диагональную матрицу $\mathbf{A}^{-1}$.
Пусть также определено правдоподобие выборки $p(\mathbf{y}|\mathbf{X}, \mathbf{w}, \boldsymbol{\Gamma}).$

\textbf{Определение} Правдоподобием модели $\mathbf{f}$ назовем следующее выражение: 
\begin{equation}
\label{eq:evidence}
	p(\mathbf{y}|\mathbf{X},\mathbf{A},\mathbf{m}, c_{\text{temp}}) = \int_{\mathbf{w}, \boldsymbol{\Gamma} } p(\mathbf{y}|\mathbf{X},\mathbf{w},  \boldsymbol{\Gamma})p(\mathbf{w}|\mathbf{A})p(\boldsymbol{\Gamma}|\mathbf{m}, c_{\text{temp}})d\mathbf{w}d\mathbf{\Gamma}.
\end{equation}

Пусть задано значение концентрации $c_{\text{temp}}$. 
Требуется найти гиперпараметры модели $\mathbf{A}, \mathbf{m}$ доставляющие максимум правдоподобия модели:
\[
    \argmax_{\mathbf{A}, \mathbf{m}}  p(\mathbf{y}|\mathbf{X},\mathbf{A},\mathbf{m}, c).
\]

\textbf{Теорема.} При $c << 0$ оптимизация  $\eqref{eq:evidence}$ эквивалентна дискретной оптимизации: $\gamma_{ij} \in \{0, 1\}, ||\gamma_i|| = 1$. 

\textbf{Доказательство.} При $c \to 0$ плотность вероятности сконцентрирована на вершинах симплекса (из оригинальной статьи). Т.к. функция вероятности ограничена, то по теореме Лебега допустим предельный переход к распределению на вершинах.  


\section{Вариационная постановка задачи}
В общем виде вычисление значения интеграла~\eqref{eq:evidence} является вычислительно сложной процедурой. В качестве приближенного значения интеграла будем использовать вариационную верхнюю оценку правдоподобия модели. Пусть заданы непрерывные параметрические распределения $q_w, q_\gamma$, аппроксимирующие апостериорные распределения $p(\mathbf{w}|\mathbf{y}, \mathbf{X}, \mathbf{A},\mathbf{m}, c)$, $p(\boldsymbol{\Gamma}|\mathbf{y}, \mathbf{X}, \mathbf{A},\mathbf{m}, c).$ Тогда верно следующее выражение:
\begin{equation}
\label{eq:elbo}
    \text{log} p(\mathbf{y}|\mathbf{X},\mathbf{A},\mathbf{m}, c)  \leq \mathsf{E}_{q_w,q_\gamma}\text{log} p(\mathbf{y}|\mathbf{X},\mathbf{w}, \boldsymbol{\Gamma}, \mathbf{A},\mathbf{m}, c) - {D_{KL}}(q_\gamma||p(\boldsymbol{\Gamma})) - {D_{KL}}(q_{w}||p(\mathbf{w})).
\end{equation}
Разница между верхней оценкой~\eqref{eq:elbo} и правдоподобием модели~\eqref{eq:evidence} определяется дивергенцией между апостериорными распределениями $p(\mathbf{w}|\mathbf{y}, \mathbf{X}, \mathbf{A},\mathbf{m}, c)$, $p(\boldsymbol{\Gamma}|\mathbf{y}, \mathbf{X}, \mathbf{A},\mathbf{m}, c).$ и вариационными распределениями  $q_w, q_\gamma$.

Определим основные величины, которые характеризуют сложность модели. \\
\textbf{Определение} Параметрической сложностью $C_w$ модели назовем наименьшую дивергенцию вариационныъ параметров  при условии априорного распределения параметров:
\[
    C_w = \argmin_\mathbf{A} D_\text{KL}\left(q|p\right).
\]
(Примечание: кажется, здесь должны учитываться параметры и структура, т.к. в априорном распределении параметров зашита зависимость от структуры).

\textbf{Определение} Структурной сложностью $C_\gamma$ модели назовем энтропию распределения структуры:
\[
    C_\gamma = \mathsf{E}_{q_\gamma} \text{log}q_\gamma.
\]
%Мотивацию сюда и определения сложности
Сформулируем основные требования к оптимизационной задаче и оптимизируемым функционалам:
\begin{enumerate}
\item Оптимизируемые функции должны быть дифференцируемы.
\item Степень регуляризации структуры и параметров должна быть контролируемой.
\item Оптимизация должна приводить к максимуму вариационной оценки.
\item Оптимизация должна позволять калибровать параметрическую сложность модели
\item Оптимизация должна позволять калибровать структурную сложность модели.
\item Оптимизация должна позволять проводить полный перебор структуры.
\item Оптимимизация должна позволять баланс регуляризации.
\end{enumerate}

Сформулируем задачу как двухуровневую задачу оптимизации. Обозначим за  $\boldsymbol{\theta}$ оптимизируемые на первом уровне величины. Обозначим за $\mathbf{h}$ величины, оптимизируемые на втором уровне.
Положим $\boldsymbol{\theta}$ равным параметрам распределений $q_w, q_\gamma$. 
Положим $\mathbf{h} = [\mathbf{A}, \mathbf{m}].$

Пусть $L$ --- приближенное значение вариационной оценки правдоподобия:
\[
    L = \beta\text{log} p(\mathbf{y}|\hat{\mathbf{w}}, \hat{\boldsymbol{\Gamma}}) - {D_{KL}}(q_\gamma||p(\boldsymbol{\Gamma})) - {D_{KL}}(q_{w}||p(\mathbf{w})),
\]
где $\hat{\mathbf{w}} \sim q_w, \quad \hat{\boldsymbol{\Gamma}} \sim q_\gamma$, $\beta$ --- коэффициент, контролирующий степень регуляризации структуры и параметров.


\textbf{Теорема}. Пусть $\beta \neq 1$ .
Тогда функция $L$ сходится по вероятности к вариационной нижней оценке правдоподобия для подвыборки  $\mathfrak{D}$ 
мощностью $\beta m$.\\
\textbf{Доказательство}. Рассмотрим произвольную подвыборку $\hat{\mathfrak{D}}$ мощностью $m_0$. Верхняя оценка правдоподобия модели для подвыборки имеет вид:
\[
 \text{log} p(\hat{\mathbf{y}}|\hat{\mathbf{X}},\mathbf{A},\mathbf{m}, c)  \leq \mathsf{E}_{q_w,q_\gamma}\text{log} p(\hat{\mathbf{y}}|\hat{\mathbf{X}},\mathbf{w}, \boldsymbol{\Gamma}, \mathbf{A},\mathbf{m}, c) - {D_{KL}}(q_\gamma||p(\boldsymbol{\Gamma})) - {D_{KL}}(q_{w}||p(\mathbf{w})).
\]

\[
\text{log} p(\hat{\mathbf{y}}|\hat{\mathbf{X}},\mathbf{w}, \boldsymbol{\Gamma}, \mathbf{A},\mathbf{m}, c) = \sum_i \text{log} p(\hat{\mathbf{y}_i}|\hat{\mathbf{x}_i},\mathbf{w}, \boldsymbol{\Gamma}, \mathbf{A},\mathbf{m}, c) \to^p_{m \to \infty} m\mathsf{E}\text{log} p(\mathbf{y}|\hat{\mathbf{x}},\mathbf{w}, \boldsymbol{\Gamma}, \mathbf{A},\mathbf{m}, c).
\]
Формула нижней оценки получается подстановкой.
~\\~\\

Пусть $Q$ --- валидационная функция:
\[
    Q(c, c_1, c_2, c_3, \mathbf{p}) = c_1\text{log} p(\mathbf{y}||\hat{\mathbf{w}}, \hat{\boldsymbol{\Gamma}}) + c_2[-{D_{KL}}(q_\gamma||p(\boldsymbol{\Gamma})) - {D_{KL}}(q_{w}||p(\mathbf{w}))] + 
\]
\[
    + c_3\sum_{p_k \in \mathbf{p}}{D_{KL}}(q_\gamma||p_k),
\]
где $\mathbf{p}$ --- заданные распределения на структурах,$c_1,c_2,c_3$ --- коэффициенты.

Сформулируем задачу поиска оптимальной модели как двухуровневую задачу.
\begin{equation}
\label{eq:optim}
	\hat{\mathbf{h}} = \argmax_{\mathbf{h} \in \mathbb{R}^h} Q( T^\eta(\boldsymbol{\theta}_0, \mathbf{h})),
\end{equation}
где $T$ --- оператор оптимизации, решающий задачу оптимизации:
\[
    L(T^\eta(\boldsymbol{\theta}_0, \mathbf{h})) \to \max.
\]


\textbf{Теорема}. Пусть $D_{KL}(q_w|p(\mathbf{w}|\mathbf{y}, \mathbf{X}, \mathbf{A},\mathbf{m}, c)) = 0, D_{KL}(q_\gamma|p(\boldsymbol{\Gamma}|\mathbf{y}, \mathbf{X}, \mathbf{A},\mathbf{m}, c)) = 0$, пусть $c_1 = 1, c_2 = 1, c_3 = 0$. Тогда оптимизация $\eqref{eq:optim}$ эквивалентна оптимизации $\eqref{eq:evidence}$.\\~\\
\textbf{Доказательство.} При соблюдении условий теоремы неравенство вариационной оценки превращается в равенство. 

\textbf{Теорема.} Пусть $c_1 = 1, c_3 = 0$. При $c_2 \to \infty$ параметрическая сложность $C_w \to 0$.

\textbf{Доказательство (идея).} При $C_w \to \infty$ априорное распределение $p(\mathbf{w})$ будет стремиться к дельта-функции. Штраф за $D_{KL}$ будет расти, параметры будут стремиться к нулю.
   

\textbf{Теорема.} Пусть $c_1 = 1,  c_3 = 0, c_2 > 0, c_2' > c_2$ и  логарифм распределения $\mathsf{E}_{q_w,q_\gamma}\text{log} p(\mathbf{y}|\mathbf{X},\mathbf{w}, \boldsymbol{\Gamma}, \mathbf{A},\mathbf{m}, c)$ является выпуклым.
Пусть $q(\mathbf{w}), q(\mathbf{w})'$ --- распределения, полученные в результате оптимизаций с различными коэффициентами $c_2, c_2'$ соответственно. Тогда  $C_w(q) \geq C_w(q').$  

\textbf{Доказательство.}
Заметим, что $q$ и $q'$ можно выразить следующим образом:
\[
    q = \arg\max_{\hat{q}: L(\hat{q}, \mathbf{A}, \boldsymbol{\Gamma}) = \max} Q( \mathbf{A}, \boldsymbol{\Gamma}, c_2),
\]
\[
    q' = \argmax_{\hat{q}: L(\hat{q}, \mathbf{A}, \boldsymbol{\Gamma})  = \max} Q( \mathbf{A}, \boldsymbol{\Gamma}, c_2').
\]

Функция $Q$ является выпуклой как сумма выпуклых функций. Отсюда справедливы следующие неравенства (по единственности точек экстремума):
\[
    \mathsf{E}_{q}\text{log} p(\mathbf{y}|\mathbf{X},\mathbf{w}, \boldsymbol{\Gamma}, \mathbf{A},\mathbf{m}, c) - c_2  D_\text{KL}(q||p)  -  \mathsf{E}_{q'}\text{log} p(\mathbf{y}|\mathbf{X},\mathbf{w}, \boldsymbol{\Gamma}, \mathbf{A},\mathbf{m}, c) + c_2  D_\text{KL}(q'||p') \geq 0,
\]
\[
    \mathsf{E}_{q'}\text{log} p(\mathbf{y}|\mathbf{X},\mathbf{w}, \boldsymbol{\Gamma}, \mathbf{A},\mathbf{m}, c) - c_2'  D_\text{KL}(q'||p')  -  \mathsf{E}_{q}\text{log} p(\mathbf{y}|\mathbf{X},\mathbf{w}, \boldsymbol{\Gamma}, \mathbf{A},\mathbf{m}, c) + c_2'  D_\text{KL}(q||p) \geq 0.
\]

Вычитая неравенства получим:
\[
    D_\text{KL}(q||p) \geq D_\text{KL}(q'||p'),
\]
\[
    \mathsf{E}_{q'}\text{log} p(\mathbf{y}|\mathbf{X},\mathbf{w}, \boldsymbol{\Gamma}, \mathbf{A},\mathbf{m}, c)  \leq \mathsf{E}_{q}\text{log} p(\mathbf{y}|\mathbf{X},\mathbf{w}, \boldsymbol{\Gamma}, \mathbf{A},\mathbf{m}, c) .
\]

С учетом полученных неравенств распишем доказываемое утверждение:
\[
    \max_p -D_\text{KL}(q||p) - \max_{p'} -D_\text{KL}(q'||p') \propto 
\]
\[ \propto\max_p - c_2' D_\text{KL}(q||p) +\mathsf{E}_{q}\text{log} p(\mathbf{y}|\mathbf{X},\mathbf{w}, \boldsymbol{\Gamma}, \mathbf{A},\mathbf{m}, c) - \mathsf{E}_{q}\text{log} p(\mathbf{y}|\mathbf{X},\mathbf{w}, \boldsymbol{\Gamma}, \mathbf{A},\mathbf{m}, c) -
\]
\[  - \max_{p'} -c_2' D_\text{KL}(q'||p')  + \mathsf{E}_{q'}\text{log} p(\mathbf{y}|\mathbf{X},\mathbf{w}, \boldsymbol{\Gamma}, \mathbf{A},\mathbf{m}, c) +\mathsf{E}_{q'}\text{log} p(\mathbf{y}|\mathbf{X},\mathbf{w}, \boldsymbol{\Gamma}, \mathbf{A},\mathbf{m}, c)     \leq 0,  
\]
что и т.д.

\textbf{Теорема} Пусть при любых выпуклых комбинациях функций $\mathbf{O}$ модель является выпуклой. Пусть $c_1,c_2 \geq 0,  c_3 \leq 0$. Пусть в качестве оценки правдоподобия выборки используется следующая:
\[
    \mathsf{E}_{q_w,q_\gamma}\text{log} p(\mathbf{y}|\mathbf{X},\mathbf{w}, \boldsymbol{\Gamma}, \mathbf{A},\mathbf{m}, c) \sim \frac{1}{N}\sum_{k=1}^N \text{log} p(\mathbf{y}|\mathbf{X},\hat{\mathbf{w}_i}, \hat{\boldsymbol{\Gamma}_i}, \mathbf{A},\mathbf{m}, c),
\]  
где $\hat{\mathbf{w}_i}, \hat{\boldsymbol{\Gamma}_i}$ получены под действием вариационной репараметризации с зафиксированной реализацией исходных случайных величин. Тогда $L$ и $Q$ являются выпуклыми функциями.

\textbf{Доказательство} По сумме выпуклых функций.

 
\textbf{Теорема} При $c \to 0$ $C_\gamma \to 0$.

\textbf{Доказательство (идея)} При $c \to 0$ $p(\boldsymbol{\Gamma})$ вероятностная масса концентрируется на вершинах симплекса. $D_\text{KL} (p||q)$ будет равняться $-\infty$ при $q_\gamma$, распределенном не на вершинах симплекса. Тогда $q_\Gamma$ становится бинарным. 


\textbf{Теорема (предварительно)} При $c \to \infty$ $C_\gamma \to \max$.

\textbf{Утверждение (предварительно, нужно развить).} Пусть $c_3 > 0, c << 0$ и все $p_k \in \mathbf{p}$ отражают распределения на вершинах симплекса. Тогда оптимизация приведет к $q_\gamma$, сконцентрированному на одной из остальных вершин симплекса.

\textbf{Утверждение (очень предварительно).} Изменение $c$ позволяет избежать ухода в локальный минимум. 

\textbf{Утверждение (очень предварительно).} Изменение $c_2$ позволяет избежать ухода в локальный минимум.

\textbf{Утверждение (очень предварительно).} Взаимосвязь структуры и параметров в prior позволяет получить <<хорошие>> модели.

\textbf{Утверждение (предварительно).} Пусть $c_1 = c_2 = c_3 = 0$. Пусть $q_w \sim \mathcal{N}(\mathbf{0}, \sigma), \sigma \sim 0$. 
Тогда оптимизация эквивалентна обычной оптимизации параметров с $l_2$ - регуляризацией.


\section{Вычислительный эксперимент}
В качестве модельного эксперимента рассматривалась задача выбора модели линейной регрессии.
Множество объектов $\mathbf{X}$ было сгенерировано из трехмерного стандартного распределения: 
\[
    \mathbf{X} \sim \mathcal{N}(0, \mathbf{I}), n = 3.
\]

Множество меток было определено следующим правилом:
\[
    \mathbf{y}= \argmax_{0,1} (\mathbf{X}_1 + \mathbf{X}_2),
\]
третья компонента не участвовала в генерации ответа.

Рассматривались четыре возможные структуры:
\begin{enumerate}
\item $f_1 = \mathbf{w}_1 \mathbf{X}_1$ (модель --- регрессия только по первому признаку), 

\item $f_2 = \mathbf{w}_2 \mathbf{X}_2$ (модель --- регрессия только по первому признаку), 

\item $f_3 = \mathbf{w}_3 \mathbf{X}_3$ (модель --- регрессия только по шумовому признаку), 

\item $f_4 = \mathbf{w}_4 \mathbf{X}$ (модель --- регрессия по всем признакам). 
\end{enumerate}

Ожидаемое поведение оптимизации:
\begin{enumerate}
\item При $c_1 = c_2 = 1 c \sim 0$ (Evidence с низкой температурой) будет произведен выбор структуры $f_4$.

\item При $c_1 = c_2 = 1, c >>0$ (Evidence с высокой температурой) будет произведен выбор двух структур с одинаковым весом: $f_1, f_2$.

\item При $c_1 = c_2 = 0, c_3 = 1, \mathbf{p}= [[0.0, 0.0, 1.0, 0.0]], c \sim 0$ (Поощряется выбор структуры с шумовой компонентой) будет произведен выбор структуры $f_4$, при снижении параметра $\beta$ выбор будет меняться в сторону $f_3$.
\end{enumerate}

\textbf{Результаты}\\
\begin{figure}
\includegraphics[width=0.5\textwidth]{Simple.png}
\caption{Evidence с низкой температурой}
\end{figure}

\begin{figure}
\includegraphics[width=0.5\textwidth]{High-temp.png}
\caption{Evidence с высокой температурой}
\end{figure}


\begin{figure}
\includegraphics[width=0.5\textwidth]{High-temp_beta.png}
\caption{Evidence с высокой температурой, $\beta = 0.01$}
\end{figure}


\begin{figure}
\includegraphics[width=0.5\textwidth]{Noise.png}
\caption{Поощрение выбора шумовой компоненты}
\end{figure}


\begin{figure}
\includegraphics[width=0.5\textwidth]{Noise_beta.png}
\caption{Поощрение выбора шумовой компоненты, $\beta = 0.01$}
\end{figure}

\end{document}  
