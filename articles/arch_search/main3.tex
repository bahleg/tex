\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{cmap}
\usepackage[T2A]{fontenc}



\usepackage{amssymb,amsfonts,amsthm,amsmath,mathtext,cite,enumerate,float}
\usepackage[russian, english]{babel}
\usepackage{graphicx}
\usepackage{tabularx}

\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{a4wide}
\usepackage{cite}

\usepackage{caption}
\usepackage{subcaption}
\usepackage{multicol}
\usepackage{multirow}

%\renewcommand{\headrulewidth}{0pt}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\begin{document}
\section{Аннотация}
В работе рассматривается задача выбора структуры модели глубокого обучения.
Модель --- это вычислительный граф, т.е. граф, в котором ребрами выступают нелинейные функции, а вершинами --- результаты дейтсвия функцией на выборку.  Каждому ребру поставлено в соответствтие множество нелинейных функций, такое что любая линейная комбинация этих функций определяет дифференцируемую функцию заданной сигнатуры. Структурой модели назовем веса линейной комбинации этих функций. 

Для нахождения оптимальной структуры предлагается ввести вероятностную интерпретацию  модели.
Проводится градиентная оптимизация параметров и гиперпараметров модели на основе вариационного вывода. Оптимизация решает двухуровневую задачу: на первом уровне проводится оптимизация нижней оценки правдоподобия модели по вариационным параметрам модели. На втором уровне проводится оптимизация гиперпараметров моделии. В качестве оптимизируемой функции для гиперпараметров модели предлагается обобщенная функция качества, позволяющая проводить оптимизацию в нескольких режимах: Add-Del, в режиме полного перебора, а также в режиме максимизации нижней оценки правдоподобия модели.


The paper presents the method of deep learning model structure selection. 
The model is a computation graph, i.e. a graph, which edges correspond to nonlinear functions and vertices correspond to the intermediate state of the dataset after application of these functions.
Each edge has a set of available nonlinear function such that any linear combination of them is a differential function of the predetermined signature. The structure of the model is a vector of weights for linear combinations of these functions.

For the optimal model strucutre selection the author proposes a probablisitic interpretation of the model. The model parameters and hyperparameters are optimized using gradient methods based on variational inference. The optimization of the model is a bi-level optimization problem. The first level corresponds to the evidence lower bound optimization with regard to  variational model parameter optimization. The second level corresponds to hyperparameter optimization. The author proposes a generalized loss function fo this level, which allows to optimize the hyperparameters in different regimes: Add-Del, bruteforce optimization and evidence lower bound optimization. 


\section{Постановка задачи}
Задана выборка  \begin{equation}\label{eq:dataset}\mathfrak{D} = \{(\mathbf{x}_i,y_i)\}, i = 1,\dots,m,\end{equation} состоящая из множества пар <<объект-метка>> $$\mathbf{x}_i \in \mathbf{X} \subset \mathbb{R}^n, \quad {y}_i \in \mathbf{y} \subset \mathbb{Y}.$$ Метка ${y}$  объекта $\mathbf{x}$ принадлежит либо множеству: ${y} \in \mathbb{Y} = \{1, \dots, Z\}$ в случае задачи классификации, где $Z$ --- число классов, либо некоторому подмножеству вещественных чисел ${y} \in \mathbb{Y}  \subseteq \mathbb{R}$ в случае задачи регрессии. Определим множество архитектур моделей глубокого обучения для дальнейшего выбора оптимальной. 

Будем рассматривать модель как граф $V,E$, ребрами $E$ которого являются функции из заданого множества функций $\mathbf{O}$, действующие на выборку, а вершинами $V$ --- промежуточные представления выборки под действием данных функций. Перейдем к формальному определению модели.
Пусть задан граф $V,E$. Пусть для каждого ребра $<i,j> \in E$ определено множество функций $\mathbf{o}(i,j)$. Граф $V, E$ с множеством функций $\mathbf{O}$ называется моделью, если функция, задаваемая рекурсивно как 
\[
    f_i(\mathbf{x}) = \sum_{j \in \text{Adj}(v_i)} o(i,j) (f_{j}(\mathbf{x})), 
\]
является непрерывной дифференцируемой функцией из $\mathbb{R}^n$ во множество $\mathbb{Y}$ при любом $o(i,j)$, являющемся линейной комбинацией функций из множества $\mathbf{o}(i,j)$.

Пусть для каждого ребра $i,j$ задан нормированный положительный вектор $\boldsymbol{\gamma}_{i,j} \in \mathbb{R+}^{|\mathbf{o}(i,j)|}$, определяющий веса функций из множества $\mathbf{o}(i,j)$.
Будем считать, что вектор $\boldsymbol{\gamma}_{i,j}$ распределен по распределению Gumbel-Softmax:
\[
    \boldsymbol{\gamma}_{i,j}  \sim \text{GS}({c}, \mathbf{m}_{i,j}).
\] 
где $c$ --- вектор концентрации распределения, $\mathbf{m}_{i,j}$ --- вектор средних. Обозначим за структуру модели $\boldsymbol{\Gamma}$ множество всех векторов $\boldsymbol{\gamma}$.
Заметим, что Gumbel-Softmax-распределение может быть заменено на распределение Дирихле. Однако в дальнейшем нам понадобится проводить градиентную оптимизацию по параметрам данного распределения. Наиболее простым вариантом для подобной оптимизации является Gumbel-Softmax.

Для более адекватной оптимизации положим априорное распределение параметров модели $\mathbf{w}$ зависящим от структуры модели $\boldsymbol{\Gamma}$.  Пусть задано соответствие $S$ между каждым параметром $\mathbf{w} \in \mathbf{W}$ и параметром структуры $\gamma_i$.
Положим распределение параметров $\mathbf{w}$ нормальным с нулевым средним и диагональной квариационной матрицей:
\[
    \mathbf{w} \sim \mathcal{N}(\mathbf{0}, \mathbf{A}^{-1} \cdot S).
\]


Пусть также определено правдоподобие выборки $p(\mathbf{y}|\mathbf{X}, \mathbf{w}, \boldsymbol{\Gamma}).$

\textbf{Определение} Правдоподобием модели $\mathbf{f}$ назовем следующее выражение: 
\begin{equation}
\label{eq:evidence}
	p(\mathbf{y}|\mathbf{X},\mathbf{A},\mathbf{m}, c) = \int_{\mathbf{w}, \boldsymbol{\Gamma} } p(\mathbf{y}|\mathbf{X},\mathbf{w},  \boldsymbol{\Gamma})p(\mathbf{w}|\mathbf{A})p(\boldsymbol{\Gamma}|\mathbf{m}, c)d\mathbf{w}d\mathbf{\Gamma}.
\end{equation}

Пусть задано значение концентрации $c$. 
Требуется найти гиперпараметры модели $\mathbf{A}, \mathbf{m}$ доставляющие максимум правдоподобия модели:
\[
    \argmax_{\mathbf{A}, \mathbf{m}}  p(\mathbf{y}|\mathbf{X},\mathbf{A},\mathbf{m}, c).
\]

\textbf{Утверждение (доказано).} При $c << 0$ оптимизация  $\eqref{eq:evidence}$ эквивалентна оптимизации дискретной оптимизации: $\boldsymbol{\gamma}_{i,j} \in 2^{|\mathbf{o}(i,j)|}$.


\section{Вариационная постановка задачи}
В общем виде вычисление значения интеграла~\eqref{eq:evidence} является вычислительно сложной процедурой. В качестве приближенного значения интеграла будем использовать вариационную верхнюю оценку правдоподобия модели. Пусть заданы непрерывные параметрические распределения $q_w, q_\gamma$, аппроксимирующие апостериорные распределения $p(\mathbf{w}|\mathbf{y}, \mathbf{X}, \mathbf{A},\mathbf{m}, c)$, $p(\boldsymbol{\Gamma}|\mathbf{y}, \mathbf{X}, \mathbf{A},\mathbf{m}, c).$ Тогда верно следующее выражение:
\begin{equation}
\label{eq:elbo}
    \text{log} p(\mathbf{y}|\mathbf{X},\mathbf{A},\mathbf{m}, c)  \leq \mathsf{E}_{q_w,q_\gamma}\text{log} p(\mathbf{y}|\mathbf{X},\mathbf{w}, \boldsymbol{\Gamma}, \mathbf{A},\mathbf{m}, c) - {D_{KL}}(q_\gamma||p(\boldsymbol{\Gamma})) - {D_{KL}}(q_{w}||p(\mathbf{w})).
\end{equation}
Разница между верхней оценкой~\eqref{eq:elbo} и правдоподобием модели~\eqref{eq:evidence} определяется дивергенцией между апостериорными распределениями $p(\mathbf{w}|\mathbf{y}, \mathbf{X}, \mathbf{A},\mathbf{m}, c)$, $p(\boldsymbol{\Gamma}|\mathbf{y}, \mathbf{X}, \mathbf{A},\mathbf{m}, c).$ и вариационными распределениями  $q_w, q_\gamma$.

%Мотивацию сюда и определения сложности
Сформулируем основные требования к оптимизационной задаче и оптимизируемым функционалам:
\begin{enumerate}
\item Оптимизируемые функции должны быть дифференцируемы.
\item Степень регуляризации структуры и параметров должна быть контролируемой.
\item Оптимизация должна приводить к максимуму вариационной оценки.
\item Оптимизация должна позволять калибровать количество эффектинвых параметров
\item Оптимизация должна позволять калибровать количество эффекитвных ребер.
\item Оптимизация должна позволять проводить полный перебор структуры.
\end{enumerate}

Положим $\boldsymbol{\theta}$ равным параметрам распределений $q_w, q_\gamma$. 
Положим $\mathbf{h} = [\mathbf{A}, \mathbf{m}].$

Пусть $L$ --- приближенное значение вариационной оценки правдоподобия:
\[
    L = \beta\text{log} p(\mathbf{y}|\hat{\mathbf{w}}, \hat{\boldsymbol{\Gamma}}) - {D_{KL}}(q_\gamma||p(\boldsymbol{\Gamma})) - {D_{KL}}(q_{w}||p(\mathbf{w})),
\]
где $\hat{\mathbf{w}} \sim q_w, \quad \hat{\boldsymbol{\Gamma}} \sim q_\gamma$, $\beta$ --- коэффициент, контролирующий степень регуляризации структуры и параметров.

\textbf{Утверждение (доказано)}. Пусть $\beta$ выражается как $\frac{m_0}{m}$, где $m_0$ --- натуральное число.
Тогда оптимизация функции $L$ эквивалентна оптимизации вариационной нижней оценки правдоподобия для подвыборки  $\mathfrak{D}$ 
мощностью $m_0$.

Пусть $Q$ --- валидационная функция:
\[
    Q(c, c_1, c_2, c_3, \mathbf{p}) = c_1\text{log} p(\mathbf{y}||\hat{\mathbf{w}}, \hat{\boldsymbol{\Gamma}}) + c_2[-{D_{KL}}(q_\gamma||p(\boldsymbol{\Gamma})) - {D_{KL}}(q_{w}||p(\mathbf{w}))] + 
\]
\[
    + c_3\sum_{p_k \in \mathbf{p}}{D_{KL}}(q_\gamma||p_k),
\]
где $\mathbf{p}$ --- заданные распределения на структурах,$c_1,c_2,c_3$ --- коэффициенты.

Сформулируем задачу поиска оптимальной модели как двухуровневую задачу.
\begin{equation}
\label{eq:optim}
	\hat{\mathbf{h}} = \argmax_{\mathbf{h} \in \mathbb{R}^h} Q( T^\eta(\boldsymbol{\theta}_0, \mathbf{h})),
\end{equation}
где $T$ --- оператор оптимизации, решающий задачу оптимизации:
\[
    L(T^\eta(\boldsymbol{\theta}_0, \mathbf{h})) \to \max.
\]


\textbf{Утверждение}. Пусть $D_{KL}(q_w|p(\mathbf{w}|\mathbf{y}, \mathbf{X}, \mathbf{A},\mathbf{m}, c)) = 0, D_{KL}(q_\gamma|p(\boldsymbol{\Gamma}|\mathbf{y}, \mathbf{X}, \mathbf{A},\mathbf{m}, c)) = 0$, пусть $c_1 = 1, c_2 = 1, c_3 = 0$. Тогда оптимизация $\eqref{eq:optim}$ эквивалентна оптимизации $\eqref{eq:evidence}$.

\textbf{Определение (предварительно)}Параметрической $\delta$-сложностью модели назовем матожидание следующей величины:
\[
    C_\text{p}(\delta, \mathbf{w}) = \mathsf{E}\sum_{w \in \mathbf{w}} I(|w| > \delta).
\] 

\textbf{Определение (предварительно)} Структурной $\delta$-сложностью модели назовем матожидание следующей величины:
\[
     C_\text{s}(\delta, \boldsymbol{\Gamma}) = \mathsf{E}\sum_{\boldsymbol{\gamma} \in \boldsymbol{\Gamma}}\sum_{\gamma_i \in \boldsymbol{\gamma}} I(\gamma_i > \delta). 
\]


\textbf{Утверждение (предварительно).} Пусть $c_1 = 1, c_3 = 0, c_2 > 0, c_2' < c_2$. Пусть $\mathbf{w}, \mathbf{w}'$ --- параметры, полученные в результате соответствющих оптимизаций. Тогда  $C_\text{p}(\delta, \mathbf{w}') \leq C_\text{p}(\delta, \mathbf{w}).$ 

\textbf{Утверждение (доказано).} Пусть $c_1 = 1, c_3 = 0$. Тогда  $C_\text{p}(\delta, \mathbf{w}(c_2)) \to_{c_2 \to \infty} 0$. 


\textbf{Утверждение (предварительно).} Пусть $c_1 = 1, c_3 = 0$. Максимум величины $C_\text{p}(\delta, \mathbf{w}(c_2))$ достигается при $c_2= 0$. 


\textbf{Утверждение (доказано).} Пусть $c_1 = c_2 = 1$. $C_\text{s}(\delta, \mathbf{w}')  \to \min$ при $c \to 0$.


\textbf{Утверждение (предварительно).} Пусть $c_1 = c_2 = 1$. $C_\text{s}(\delta, \mathbf{w}')  \to \max$ при $c \to \infty$.

\textbf{Утверждение (предварительно, нужно развить).} Пусть $c_3 > 0, c << 0$ и все $p_k \in \mathbf{p}$ отражают распределения на вершинах симплекса. Тогда оптимизация приведет к $q_\gamma$, сконцентрированному на одной из остальных вершин симплекса.

\textbf{Утверждение (очень предварительно).} Изменение $c$ позволяет избежать ухода в локальный минимум. 

\textbf{Утверждение (очень предварительно).} Изменение $c_2$ позволяет избежать ухода в локальный минимум.

\textbf{Утверждение (очень предварительно).} Взаимосвязь структуры и параметров в prior позволяет получить <<хорошие>> модели.

\textbf{Утверждение (предварительно).} Пусть $c_1 = c_2 = c_3 = 0$. Пусть $q_w \sim \mathcal{N}(\mathbf{0}, \sigma), \sigma \sim 0$. 
Тогда оптимизация эквивалентна обычной оптимизации параметров с $l_2$ - регуляризацией.

Далее будем рассматривать $q_w \sim \mathcal{N}(\mathbf{0}, \mathbf{A}_q^{-1}), \quad q_\gamma \sim \text{Gumbel-Softmax}(\mathbf{g}, \tau).$

\section{Вычислительный эксперимент}
В качестве модельного эксперимента рассматривалась задача выбора модели линейной регрессии.
Множество объектов $\mathbf{X}$ было сгенерировано из трехмерного стандартного распределения: 
\[
    \mathbf{X} \sim \mathcal{N}(0, \mathbf{I}), n = 3.
\]

Множество меток было определено следующим правилом:
\[
    \mathbf{y}= \argmax_{0,1} (\mathbf{X}_1 + \mathbf{X}_2),
\]
третья компонента не участвовала в генерации ответа.

Рассматривались четыре возможные структуры:
\begin{enumerate}
\item $f_1 = \mathbf{w}_1 \mathbf{X}_1$ (модель --- регрессия только по первому признаку), 

\item $f_2 = \mathbf{w}_2 \mathbf{X}_2$ (модель --- регрессия только по первому признаку), 

\item $f_3 = \mathbf{w}_3 \mathbf{X}_3$ (модель --- регрессия только по шумовому признаку), 

\item $f_4 = \mathbf{w}_4 \mathbf{X} (модель --- регрессия по всем признакам). $
\end{enumerate}

Ожидаемое поведение оптимизации:
\begin{enumerate}
\item При $c_1 = c_2 = 1 c \sim 0$ (Evidence с низкой температурой) будет произведен выбор структуры $f_4$.

\item При $c_1 = c_2 = 1, c >>0$ (Evidence с высокой температурой) будет произведен выбор двух структур с одинаковым весом: $f_1, f_2$.

\item При $c_1 = c_2 = 0, c_3 = 1, \mathbf{p}= [[0.0, 0.0, 1.0, 0.0]], c \sim 0$ (Поощряется выбор структуры с шумовой компонентой) будет произведен выбор структуры $f_4$, при снижении параметра $\beta$ выбор будет меняться в сторону $f_3$.
\end{enumerate}

\textbf{Результаты}\\
\begin{figure}
\includegraphics[width=0.5\textwidth]{Simple.png}
\caption{Evidence с низкой температурой}
\end{figure}

\begin{figure}
\includegraphics[width=0.5\textwidth]{High-temp.png}
\caption{Evidence с высокой температурой}
\end{figure}


\begin{figure}
\includegraphics[width=0.5\textwidth]{High-temp_beta.png}
\caption{Evidence с высокой температурой, $\beta = 0.01$}
\end{figure}


\begin{figure}
\includegraphics[width=0.5\textwidth]{Noise.png}
\caption{Поощрение выбора шумовой компоненты}
\end{figure}


\begin{figure}
\includegraphics[width=0.5\textwidth]{Noise_beta.png}
\caption{Поощрение выбора шумовой компоненты, $\beta = 0.01$}
\end{figure}

\end{document}  
