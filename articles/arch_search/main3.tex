\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{cmap}
\usepackage[T2A]{fontenc}



\usepackage{amssymb,amsfonts,amsthm,amsmath,mathtext,cite,enumerate,float}
\usepackage[russian, english]{babel}
\usepackage{graphicx}
\usepackage{tabularx}

\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{a4wide}
\usepackage{cite}

\usepackage{caption}
\usepackage{subcaption}
\usepackage{multicol}
\usepackage{multirow}

%\renewcommand{\headrulewidth}{0pt}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\begin{document}
\section{Аннотация}
В работе рассматривается задача выбора структуры модели глубокого обучения. Модель --- вычислительный граф со множеством операций на ребрах, такой что любой выбор операций порождает дифференцируемую функцию заданной сигнатуры. Структурой модели назовем набор выбранных операций с их весами. Для определения оптимальной структуры предлагается ввести вероятностную интерпретацию структуры модели и провести оптимизацию на основе вариационного вывода. В качестве внешнего критерия качества выбора структуры предлагается обобщенная функция качества, позволяющая проводить оптимизацию в нескольких режимах: add-del, полный перебор, максимизация правдоподобия модели.


\section{Постановка задачи}
Задана выборка  \begin{equation}\label{eq:dataset}\mathfrak{D} = \{(\mathbf{x}_i,y_i)\}, i = 1,\dots,m,\end{equation} состоящая из множества пар <<объект-метка>> $$\mathbf{x}_i \in \mathbf{X} \subset \mathbb{R}^n, \quad {y}_i \in \mathbf{y} \subset \mathbb{Y}.$$ Метка ${y}$  объекта $\mathbf{x}$ принадлежит либо множеству: ${y} \in \mathbb{Y} = \{1, \dots, Z\}$ в случае задачи классификации, где $Z$ --- число классов, либо некоторому подмножеству вещественных чисел ${y} \in \mathbb{Y}  \subseteq \mathbb{R}$ в случае задачи регрессии. Определим множество архитектур моделей глубокого обучения для дальнейшего выбора оптимальной. 

Будем рассматривать модель как граф $V,E$, ребрами $E$ которого являются функции из заданого множества функций $\mathbf{O}$, действующие на выборку, а вершинами $V$ --- промежуточные представления выборки под действием данных функций. Перейдем к формальному определению модели.
Пусть задан граф $V,E$. Пусть для каждого ребра $<i,j> \in E$ определено множество функций $\mathbf{o}(i,j)$. Граф $V, E$ с множеством функций $\mathbf{O}$ называется моделью, если функция, задаваемая рекурсивно как 
\[
    f_i(\mathbf{x}) = \sum_{j \in \text{Adj}(v_i)} o(i,j) (f_{j}(\mathbf{x})), 
\]
является непрерывной дифференцируемой функцией из $\mathbb{R}^n$ во множество $\mathbb{Y}$ при любом $o(i,j)$, являющемся линейной комбинацией функций из множества $\mathbf{o}(i,j)$.

Пусть для каждого ребра $i,j$ задан нормированный положительный вектор $\boldsymbol{\gamma}_{i,j} \in \mathbb{R+}^{|\mathbf{o}(i,j)|}$, определяющий веса функций из множества $\mathbf{o}(i,j)$.
Будем считать, что вектор $\boldsymbol{\gamma}_{i,j}$ распределен по распределению Gumbel-Softmax:
\[
    \boldsymbol{\gamma}_{i,j}  \sim \text{GS}({c}, \mathbf{m}_{i,j}).
\] 
где $c$ --- вектор концентрации распределения, $\mathbf{m}_{i,j}$ --- вектор средних. Обозначим за структуру модели $\boldsymbol{\Gamma}$ множество всех векторов $\boldsymbol{\gamma}$.
Заметим, что Gumbel-Softmax-распределение может быть заменено на распределение Дирихле. Однако в дальнейшем нам понадобится проводить градиентную оптимизацию по параметрам данного распределения. Наиболее простым вариантом для подобной оптимизации является Gumbel-Softmax.

Для более адекватной оптимизации положим априорное распределение параметров модели $\mathbf{w}$ зависящим от структуры модели $\boldsymbol{\Gamma}$.  Пусть задано соответствие $S$ между каждым параметром $\mathbf{w} \in \mathbf{W}$ и параметром структуры $\gamma_i$.
Положим распределение параметров $\mathbf{w}$ нормальным с нулевым средним и диагональной квариационной матрицей:
\[
    \mathbf{w} \sim \mathcal{N}(\mathbf{0}, \mathbf{A}^{-1} \cdot S).
\]


Пусть также определено правдоподобие выборки $p(\mathbf{y}|\mathbf{X}, \mathbf{w}, \boldsymbol{\Gamma}).$

\textbf{Определение} Правдоподобием модели $\mathbf{f}$ назовем следующее выражение: 
\begin{equation}
\label{eq:evidence}
	p(\mathbf{y}|\mathbf{X},\mathbf{A},\mathbf{m}, c) = \int_{\mathbf{w}, \boldsymbol{\Gamma} } p(\mathbf{y}|\mathbf{X},\mathbf{w},  \boldsymbol{\Gamma})p(\mathbf{w}|\mathbf{A})p(\boldsymbol{\Gamma}|\mathbf{m}, c)d\mathbf{w}d\mathbf{\Gamma}.
\end{equation}

Пусть задано значение концентрации $c$. 
Требуется найти гиперпараметры модели $\mathbf{A}, \mathbf{m}$ доставляющие максимум правдоподобия модели:
\[
    \argmax_{\mathbf{A}, \mathbf{m}}  p(\mathbf{y}|\mathbf{X},\mathbf{A},\mathbf{m}, c).
\]

\textbf{Утверждение (доказано).} При $c << 0$ оптимизация  $\eqref{eq:evidence}$ эквивалентна оптимизации дискретной оптимизации: $\boldsymbol{\gamma}_{i,j} \in 2^{|\mathbf{o}(i,j)|}$.


\section{Вариационная постановка задачи}
В общем виде вычисление значения интеграла~\eqref{eq:evidence} является вычислительно сложной процедурой. В качестве приближенного значения интеграла будем использовать вариационную верхнюю оценку правдоподобия модели. Пусть заданы непрерывные параметрические распределения $q_w, q_\gamma$, аппроксимирующие апостериорные распределения $p(\mathbf{w}|\mathbf{y}, \mathbf{X}, \mathbf{A},\mathbf{m}, c)$, $p(\boldsymbol{\Gamma}|\mathbf{y}, \mathbf{X}, \mathbf{A},\mathbf{m}, c).$ Тогда верно следующее выражение:
\begin{equation}
\label{eq:elbo}
    \text{log} p(\mathbf{y}|\mathbf{X},\mathbf{A},\mathbf{m}, c)  \leq \mathsf{E}_{q_w,q_\gamma}\text{log} p(\mathbf{y}|\mathbf{X},\mathbf{w}, \boldsymbol{\Gamma}, \mathbf{A},\mathbf{m}, c) - {D_{KL}}(q_\gamma||p(\boldsymbol{\Gamma})) - {D_{KL}}(q_{w}||p(\mathbf{w})).
\end{equation}
Разница между верхней оценкой~\eqref{eq:elbo} и правдоподобием модели~\eqref{eq:evidence} определяется дивергенцией между апостериорными распределениями $p(\mathbf{w}|\mathbf{y}, \mathbf{X}, \mathbf{A},\mathbf{m}, c)$, $p(\boldsymbol{\Gamma}|\mathbf{y}, \mathbf{X}, \mathbf{A},\mathbf{m}, c).$ и вариационными распределениями  $q_w, q_\gamma$.

%Мотивацию сюда и определения сложности
Сформулируем основные требования к оптимизационной задаче и оптимизируемым функционалам:
\begin{enumerate}
\item Оптимизируемые функции должны быть дифференцируемы.
\item Степень регуляризации структуры и параметров должна быть контролируемой.
\item Оптимизация должна приводить к максимуму вариационной оценки.
\item Оптимизация должна позволять калибровать количество эффектинвых параметров
\item Оптимизация должна позволять калибровать количество эффекитвных ребер.
\item Оптимизация должна позволять проводить полный перебор структуры.
\end{enumerate}

Положим $\boldsymbol{\theta}$ равным параметрам распределений $q_w, q_\gamma$. 
Положим $\mathbf{h} = [\mathbf{A}, \mathbf{m}].$

Пусть $L$ --- приближенное значение вариационной оценки правдоподобия:
\[
    L = \beta\text{log} p(\mathbf{y}|\hat{\mathbf{w}}, \hat{\boldsymbol{\Gamma}}) - {D_{KL}}(q_\gamma||p(\boldsymbol{\Gamma})) - {D_{KL}}(q_{w}||p(\mathbf{w})),
\]
где $\hat{\mathbf{w}} \sim q_w, \quad \hat{\boldsymbol{\Gamma}} \sim q_\gamma$, $\beta$ --- коэффициент, контролирующий степень регуляризации структуры и параметров.

\textbf{Утверждение (доказано)}. Пусть $\beta$ выражается как $\frac{m_0}{m}$, где $m_0$ --- натуральное число.
Тогда оптимизация функции $L$ эквивалентна оптимизации вариационной нижней оценки правдоподобия для подвыборки  $\mathfrak{D}$ 
мощностью $m_0$.

Пусть $Q$ --- валидационная функция:
\[
    Q(c, c_1, c_2, c_3, \mathbf{p}) = c_1\text{log} p(\mathbf{y}||\hat{\mathbf{w}}, \hat{\boldsymbol{\Gamma}}) + c_2[-{D_{KL}}(q_\gamma||p(\boldsymbol{\Gamma})) - {D_{KL}}(q_{w}||p(\mathbf{w}))] + 
\]
\[
    + c_3\sum_{p_k \in \mathbf{p}}{D_{KL}}(q_\gamma||p_k),
\]
где $\mathbf{p}$ --- заданные распределения на структурах,$c_1,c_2,c_3$ --- коэффициенты.

Сформулируем задачу поиска оптимальной модели как двухуровневую задачу.
\begin{equation}
\label{eq:optim}
	\hat{\mathbf{h}} = \argmax_{\mathbf{h} \in \mathbb{R}^h} Q( T^\eta(\boldsymbol{\theta}_0, \mathbf{h})),
\end{equation}
где $T$ --- оператор оптимизации, решающий задачу оптимизации:
\[
    L(T^\eta(\boldsymbol{\theta}_0, \mathbf{h})) \to \max.
\]


\textbf{Утверждение}. Пусть $D_{KL}(q_w|p(\mathbf{w}|\mathbf{y}, \mathbf{X}, \mathbf{A},\mathbf{m}, c)) = 0, D_{KL}(q_\gamma|p(\boldsymbol{\Gamma}|\mathbf{y}, \mathbf{X}, \mathbf{A},\mathbf{m}, c)) = 0$, пусть $c_1 = 1, c_2 = 1, c_3 = 0$. Тогда оптимизация $\eqref{eq:optim}$ эквивалентна оптимизации $\eqref{eq:evidence}$.

\textbf{Определение (предварительно)}Параметрической $\delta$-сложностью модели назовем матожидание следующей величины:
\[
    C_\text{p}(\delta, \mathbf{w}) = \mathsf{E}\sum_{w \in \mathbf{w}} I(|w| > \delta).
\] 

\textbf{Определение (предварительно)} Структурной $\delta$-сложностью модели назовем матожидание следующей величины:
\[
     C_\text{s}(\delta, \boldsymbol{\Gamma}) = \mathsf{E}\sum_{\boldsymbol{\gamma} \in \boldsymbol{\Gamma}}\sum_{\gamma_i \in \boldsymbol{\gamma}} I(\gamma_i > \delta). 
\]


\textbf{Утверждение (предварительно).} Пусть $c_1 = 1, c_3 = 0, c_2 > 0, c_2' < c_2$. Пусть $\mathbf{w}, \mathbf{w}'$ --- параметры, полученные в результате соответствющих оптимизаций. Тогда  $C_\text{p}(\delta, \mathbf{w}') \leq C_\text{p}(\delta, \mathbf{w}).$ 

\textbf{Утверждение (доказано).} Пусть $c_1 = 1, c_3 = 0$. Тогда  $C_\text{p}(\delta, \mathbf{w}(c_2)) \to_{c_2 \to \infty} 0$. 


\textbf{Утверждение (предварительно).} Пусть $c_1 = 1, c_3 = 0$. Максимум величины $C_\text{p}(\delta, \mathbf{w}(c_2))$ достигается при $c_2= 0$. 


\textbf{Утверждение (доказано).} Пусть $c_1 = c_2 = 1$. $C_\text{s}(\delta, \mathbf{w}')  \to \min$ при $c \to 0$.


\textbf{Утверждение (предварительно).} Пусть $c_1 = c_2 = 1$. $C_\text{s}(\delta, \mathbf{w}')  \to \max$ при $c \to \infty$.

\textbf{Утверждение (предварительно, нужно развить).} Пусть $c_3 > 0, c << 0$ и все $p_k \in \mathbf{p}$ отражают распределения на вершинах симплекса. Тогда оптимизация приведет к $q_\gamma$, сконцентрированному на одной из остальных вершин симплекса.

\textbf{Утверждение (очень предварительно).} Изменение $c$ позволяет избежать ухода в локальный минимум. 

\textbf{Утверждение (очень предварительно).} Изменение $c_2$ позволяет избежать ухода в локальный минимум.

\textbf{Утверждение (очень предварительно).} Взаимосвязь структуры и параметров в prior позволяет получить <<хорошие>> модели.

\textbf{Утверждение (предварительно).} Пусть $c_1 = c_2 = c_3 = 0$. Пусть $q_w \sim \mathcal{N}(\mathbf{0}, \sigma), \sigma \sim 0$. 
Тогда оптимизация эквивалентна обычной оптимизации параметров с $l_2$ - регуляризацией.

Далее будем рассматривать $q_w \sim \mathcal{N}(\mathbf{0}, \mathbf{A}_q^{-1}), \quad q_\gamma \sim \text{Gumbel-Softmax}(\mathbf{g}, \tau).$

\section{Вычислительный эксперимент}
В качестве модельного эксперимента рассматривалась задача выбора модели линейной регрессии.
Множество объектов $\mathbf{X}$ было сгенерировано из трехмерного стандартного распределения: 
\[
    \mathbf{X} \sim \mathcal{N}(0, \mathbf{I}), n = 3.
\]

Множество меток было определено следующим правилом:
\[
    \mathbf{y}= \argmax_{0,1} (\mathbf{X}_1 + \mathbf{X}_2),
\]
третья компонента не участвовала в генерации ответа.

Рассматривались четыре возможные структуры:
\begin{enumerate}
\item $f_1 = \mathbf{w}_1 \mathbf{X}_1$ (модель --- регрессия только по первому признаку), 

\item $f_2 = \mathbf{w}_2 \mathbf{X}_2$ (модель --- регрессия только по первому признаку), 

\item $f_3 = \mathbf{w}_3 \mathbf{X}_3$ (модель --- регрессия только по шумовому признаку), 

\item $f_4 = \mathbf{w}_4 \mathbf{X} (модель --- регрессия по всем признакам). $
\end{enumerate}

Ожидаемое поведение оптимизации:
\begin{enumerate}
\item При $c_1 = c_2 = 1 c \sim 0$ (Evidence с низкой температурой) будет произведен выбор структуры $f_4$.

\item При $c_1 = c_2 = 1, c >>0$ (Evidence с высокой температурой) будет произведен выбор двух структур с одинаковым весом: $f_1, f_2$.

\item При $c_1 = c_2 = 0, c_3 = 1, \mathbf{p}= [[0.0, 0.0, 1.0, 0.0]], c \sim 0$ (Поощряется выбор структуры с шумовой компонентой) будет произведен выбор структуры $f_4$, при снижении параметра $\beta$ выбор будет меняться в сторону $f_3$.
\end{enumerate}

\textbf{Результаты}\\
\begin{figure}
\includegraphics[width=0.5\textwidth]{Simple.png}
\caption{Evidence с низкой температурой}
\end{figure}

\begin{figure}
\includegraphics[width=0.5\textwidth]{High-temp.png}
\caption{Evidence с высокой температурой}
\end{figure}


\begin{figure}
\includegraphics[width=0.5\textwidth]{High-temp_beta.png}
\caption{Evidence с высокой температурой, $\beta = 0.01$}
\end{figure}


\begin{figure}
\includegraphics[width=0.5\textwidth]{Noise.png}
\caption{Поощрение выбора шумовой компоненты}
\end{figure}


\begin{figure}
\includegraphics[width=0.5\textwidth]{Noise_beta.png}
\caption{Поощрение выбора шумовой компоненты, $\beta = 0.01$}
\end{figure}

\end{document}  
