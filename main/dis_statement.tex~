\newpage{}
\addcontentsline{toc}{section}{Постановка задачи}
\chapter*{Постановка задачи}


\section{Метаоптимизация}

\paragraph{Теоретические основы метоотимизации}
\paragraph{Эволюционные и переборные алгоритмы метаоптимизации}
\paragraph{Обучение с подкреплением}
\paragraph{Алгоритмы наращивания и прореживания параметров модели}
Один из возможных методов выбора структуры модели был предложен в работе.
http://yann.lecun.com/exdb/publis/pdf/lecun-90b.pdf
Предлагалось удалять неинформативные параметры модели, где в качестве показателя информативности выступал следующий фнуционал. 
В работе была продолжена данная идея, для вычисления страшной формулы был использован полный гессиан функции ошбики:
https://authors.library.caltech.edu/54981/1/Optimal%20Brain%20Surgeon%20and%20general%20network%20pruning.pdf
https://arxiv.org/pdf/1705.07565.pdf

В работе  
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.704.7138&rep=rep1&type=pdf
был предложен метод, основанный на получении вариационной нижней оценки правдоподобия модели. В качестве критерия информативности параметра выступало отношение правдоподобия параметра при различных априорных распределениях. Идея даннного метода была развита в работе 
https://arxiv.org/pdf/1705.08665.pdf, где также используются вариационные методы. В отличие от предыдущей работы, в данной работе рассматривается ряд априорных распределений параметров, позволяющих прореживать модели более эффективно. 

Показать нормальное распределение и коши.

Смежной темой к прореижванию моделей выступает компрессия нейросетей. Основным отличием задачи прореживания и компрессии выступает эксплуатационное требование: если прореживание используется для получения оптимальной и наиболее устойчивой модели, то компрессия часто производится для сохранения памяти и основных эксплуатационных характеристик исходной модели.  В работе https://arxiv.org/pdf/1506.02626.pdf
предлагается итеритавиное использование регуляризации типа DRopOut для прореживания модели. 
В работе https://arxiv.org/pdf/1702.03044.pdf используются методы снижения вычислительной точности представления парамеров моедли. 
В работе https://arxiv.org/pdf/1510.00149.pdf предлагается метод компрессии, основанный на кластеризации значений параметров модели и представлении их в сжатом виде на основе кодов Хаффмана.

В работах предлагается наращивание моделей, основанное на бустинге. В работе пна каждом шаге построения проверяется две альтернативы:
1. Сделать модель шире
2. Сделать модель глубже
Построение модели заканчивается при условии снижении радемахереовской сложности:
фомулы

TODO:
MIO?
Возмущение

\section{Адаптивное обучение}
\section{Байесовские методы порождения и выбора моделей}
\section{Способы прогнозирования графовых структур}
\section{Эвристические и прикладные методы}
