\documentclass[10pt,pdf,utf8,russian,aspectratio=169]{beamer}
\usepackage[T2A]{fontenc}
\usepackage[english,russian]{babel}
\usepackage{subfig}
\usepackage{graphicx}
\usepackage{multicol}
\usepackage{cancel}
%
% Choose how your presentation looks.
%
% For more themes, color themes and font themes, see:
% http://deic.uab.es/~iblanes/beamer_gallery/index_by_theme.html
%
\mode<presentation>
{
  \usetheme{Boadilla}      % or try Darmstadt, Madrid, Warsaw, ...
  \usecolortheme{seagull} % or try albatross, beaver, crane, ..

  \usefonttheme{structurebold}  % or try serif, structurebold, ...
  \setbeamertemplate{navigation symbols}{}
  \setbeamertemplate{caption}[numbered]
} 

\captionsetup[subfloat]{labelformat=empty}
\title[Оптимизация гиперпараметров]{Оптимизация гиперпараметров градиентными методами}
\author{Бахтеев Олег}
\institute{МФТИ}
\date{04.04.2018}
%\renewcommand{\headrulewidth}{0pt}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\begin{document}

\begin{frame}
  \titlepage
\end{frame}



\begin{frame}{Градиентные методы: зачем?}
\begin{columns}
\begin{column}{0.5\textwidth}
\begin{itemize}
\item Гиперпараметры --- параметры распределения параметров модели.
\item Основные методы оптимизации не позволяют проводить оптимизацию большого количества гиперпараметров (>10).
\item Решение проблемы --- использование градиентного спуска для гиперпараметров.
\end{itemize}
\end{column}
\begin{column}{0.4\textwidth}
\begin{figure}[h]
\includegraphics[width=\textwidth]{./gp.png}
\caption*{Shahriari et. al, 2016. Пример работы гауссового процесса.}
\end{figure}

\end{column}
\end{columns}

\end{frame}



\begin{frame}{Постановка задачи}
Задана дифференцируемая по параметрам модель, приближающая зависимую переменную~$y$:
\[
	f:\mathbb{R}^n \to \mathbb{Y}, \quad \mathbf{w} \in \mathbb{R}^u.
\]
Функция $f$ задает правдоподобие выборки $\text{log}p(\mathbf{y}|\mathbf{X}, f)$. 

Пусть также задано априорное распределение параметров: $$\mathbf{w} \sim \mathcal{N}(\mathbf{0}, \mathbf{A}^{-1}),$$
где $\mathbf{A}^{-1} = \text{diag}[\alpha_1, \dots, \alpha_u]^{-1}$ --- матрица ковариаций диагонального вида, определеяемая \textit{гиперпараметрами} $[\alpha_1, \dots, \alpha_u]$.
\end{frame}
\begin{frame}{Кросс-валидация}
Разобьем выборку $\mathfrak{D}$ на $k$ равных частей:
\[
\mathfrak{D} = \mathfrak{D}_1 \sqcup \dots \sqcup \mathfrak{D}_k.
\]


Запустим $k$ оптимизаций модели, каждую на своей части выборки. Положим $\boldsymbol{\theta} = [\mathbf{w}_1, \dots, \mathbf{w}_k]$, где $\mathbf{w}_1, \dots, \mathbf{w}_k$ --- параметры модели при оптимизации $k$.
 
Пусть $L$ --- функция потерь:
\begin{equation}
\label{eq:cv}
L(\boldsymbol{\theta}, \mathbf{h}) = -\frac{1}{k}\sum_{q=1}^k \bigl(\frac{k}{k-1}\text{log}p(\mathbf{y} \setminus \mathbf{y}_q|\mathbf{X}\setminus \mathbf{X}_q, \mathbf{w}_q) + \text{log}p(\mathbf{w}_q|\mathbf{A})\bigr).
\end{equation}

Пусть $Q$ --- функция качества модели:
\[
Q(\boldsymbol{\theta}, \mathbf{h}) = \frac{1}{k}\sum_{q=1}^k k\text{log}p(\mathbf{y}_q|\mathbf{X}_q, \mathbf{w}_q).
\]

\end{frame}


\begin{frame}{Формальная постановка задачи}
Задана дифференцируемая по параметрам модель, приближающая зависимую переменную~$y$:
\[
	f:\mathbb{R}^n \to \mathbb{Y}, \quad \mathbf{w} \in \mathbb{R}^u.
\]

Пусть $\boldsymbol{\theta} \in \mathbb{R}^s$ --- множество всех оптимизируемых параметров.\\
$L(\boldsymbol{\theta}, \mathbf{h})$ ---  дифференцируемая функция потерь  по которой производится оптимизация функции ${f}$. \\
$Q(\boldsymbol{\theta}, \mathbf{h})$ ---  дифференцируемая функция определяющая итоговое качество модели ${f}$ и приближающая интеграл.\\

Требуется найти параметры $\hat{\boldsymbol{\theta}}$ и гиперпараметры $\hat{\mathbf{h}}$ модели, доставляющие минимум следующему функционалу:
\[
\hat{\mathbf{h}} = \argmax_{\mathbf{h} \in \mathbb{R}^h} Q(\hat{\boldsymbol{\theta}}(\mathbf{h}), \mathbf{h}),
\]
\[
	\hat{\boldsymbol{\theta}}(\mathbf{h}) =  \argmin_{\boldsymbol{\theta} \in \mathbb{R}^s} L(\boldsymbol{\theta}, \mathbf{h}).
\]
\end{frame}
\iffalse
\begin{frame}{Байесовский вывод}
\textit{Первый уровень:}
\[
\hat{\boldsymbol{\theta}} = \argmax \bigl(-L(\boldsymbol{\theta}, \mathbf{h})\bigr) = p(\mathbf{w}|\mathbf{X}, \mathbf{y}, \mathbf{A}) = \frac{p(\mathbf{y}|\mathbf{X},\mathbf{w})p(\mathbf{w}|\mathbf{A})}{p(\mathbf{y}|\mathbf{X},\mathbf{A})}.
\]
\textit{Второй уровень:}
\[
p(\mathbf{A}|\mathbf{X}, \mathbf{y}) \propto p(\mathbf{y}|\mathbf{X},\mathbf{A})p(\mathbf{A}),
\]

Полагая распределение параметров $p(\mathbf{A})$ равномерным на некоторой большой окрестности, получим задачу оптимизации гиперпараметров:
\begin{equation}
\label{eq:bayes2}
	Q(\boldsymbol{\theta}, \mathbf{h}) = p(\mathbf{y}|\mathbf{X},\mathbf{A}) = \int_{\mathbf{w} \in \mathbb{R}^u} p(\mathbf{y}|\mathbf{X}, \mathbf{w}) p(\mathbf{w}|\mathbf{A}) \to \max_{[\alpha_1, \dots, \alpha_u] \in \mathbb{R}^{n}}.
\end{equation}
\end{frame}
\fi

\begin{frame}{Байесовый подход к сложности}
Правдоподобие модели (``Evidence''):
\[
	p(\mathbf{y}|\mathbf{f}) = \int_\mathbf{w} p(\mathbf{y}|\mathbf{X},\mathbf{w})p(\mathbf{w}|\mathbf{A}) d\mathbf{w}.
\]


\begin{figure}
  \centering
  \subfloat[Схема выбора модели по правдоподобию]{\includegraphics[width=0.4\textwidth]{evidence.pdf}} 
 \subfloat[Пример: полиномы]{\includegraphics[width=0.4\textwidth]{example.pdf}}
\label{fig:1}\qquad

\end{figure}


\end{frame}


\begin{frame}{Вариационная нижняя оценка}
Пусть задано непрерывное распределение $q$. 
Тогда 
$$
\text{log}~p(\mathbf{y}|\mathbf{X},\mathbf{w})  = \int_{\mathbf{w}} q(\mathbf{w})\text{log}~\frac{p(\mathbf{y},\mathbf{w}|\mathbf{X},\mathbf{A})}{q(\mathbf{w})}d\mathbf{w} + \text{D}_\text{KL}  (q(\mathbf{w})||p(\mathbf{w}| \mathbf{y}, \mathbf{X}, \mathbf{A})) \geq	
$$
$$
\geq \int_{\mathbf{w}} q(\mathbf{w})\text{log}~\frac{p(\mathbf{y},  \mathbf{w}|\mathbf{X}, \mathbf{A})}{q(\mathbf{w})}d\mathbf{w} =
$$

$$
= -\text{D}_\text{KL} (q(\mathbf{w})||p(\mathbf{w}|\mathbf{A})) + \int_{\mathbf{w}} q(\mathbf{w})\text{log}~{p(\mathbf{y}|\mathbf{X}, \mathbf{w},\mathbf{A})} d \mathbf{w},
$$
где $$\text{D}_\text{KL}(q(\mathbf{w})||p(\mathbf{w} |\mathbf{A})) = -\int_{\mathbf{w}} q(\mathbf{w})\text{log}~\frac{p(\mathbf{w} | \mathbf{A})}{q(\mathbf{w})}d\mathbf{w}.$$

\end{frame}


\begin{frame}{Evidence: нормальное распределение}
\textbf{``Обычная'' функция потерь:}\\
$$
L = \textcolor{red}{\sum_{\mathbf{x}, \mathbf{y} \in \mathfrak{D}} - \text{log}p(\mathbf{y}|\mathbf{x}, \mathbf{w})} + \textcolor{blue}{\lambda||\mathbf{w}||_2^2}.
$$\\~\\

% если n - константна.
\textbf{Вариационный вывод при $p(\mathbf{w}|\mathbf{f}) \sim \mathcal{N}(\mathbf{0}, \mathbf{1})$:}\\
$$
L =   \textcolor{red}{\sum_{\mathbf{x}, \mathbf{y} \in \mathfrak{D}} \text{log}~p(\mathbf{y}|\mathbf{x}, \hat{\mathbf{w}})} +
 \textcolor{blue}{\frac{1}{2} \bigl( \text{tr} (\mathbf{A}^{-1}_q) + \boldsymbol{\mu}_q^\text{T}\mathbf{A}^{-1}\boldsymbol{\mu}_q  - \text{ln}~|\mathbf{A}^{-1}_q| \bigr)},
$$\\$$\hat{\mathbf{w}} \sim q = \mathcal{N}(\boldsymbol{\mu}_q, \mathbf{A}^{-1}_q).$$~\\


\end{frame}

\begin{frame}{Вариационная оценка: оптимизация гиперпараметров}
Пусть $L=-Q$:
\begin{equation} 
\label{eq:elbo}
\text{log}~p(\mathbf{y}|\mathbf{X},\mathbf{A})  
\geq 
\sum_{\mathbf{x},y} \text{log}~p({y}|\mathbf{x}, \hat{\mathbf{w}}) - D_\text{KL}\bigl(q (\mathbf{w}) || p (\mathbf{w}|\mathbf{A})\bigr) = -L(\boldsymbol{\theta}, \mathbf{h}) = Q((\boldsymbol{\theta}, \mathbf{h}),
\end{equation}
где $q$ --- нормальное распределение с диагональной матрицей ковариаций:
\begin{equation}
\label{eq:diag}
	q \sim \mathcal{N}(\boldsymbol{\mu}_q, \mathbf{A}^{-1}_q),
\end{equation}
$$
D_\text{KL}\bigl(q (\mathbf{w}) || p (\mathbf{w}|\mathbf{f})\bigr) = \frac{1}{2} \bigl( \text{Tr} [\mathbf{A}\mathbf{A}^{-1}_q] + (\boldsymbol{\mu} - \boldsymbol{\mu}_q)^\mathsf{T}\mathbf{A}(\boldsymbol{\mu} - \boldsymbol{\mu}_q) - u +\text{ln}~|\mathbf{A}^{-1}| - \text{ln}~|\mathbf{A}_q^{-1}| \bigr).
$$

В качестве оптимизируемых параметров $\boldsymbol{\theta}$ выступают параметры распределения $q$:
\[
\boldsymbol{\theta} = [\alpha_1, \dots, \alpha_u, {\mu}_1,\dots,{\mu}_u].
\]


\end{frame}


\begin{frame}{Формальная постановка задачи: градиентная оптимизация}
\begin{block}{Определение}
Оператором $T$ назовем оператор стохастического градиентного спуска, производящий $\eta$ шагов оптимизации:
\begin{equation}
\label{eq:gd}
	 \hat{\boldsymbol{\theta}} = T \circ T \circ \dots \circ T(\boldsymbol{\theta}_0, \mathbf{h}) = T^\eta(\boldsymbol{\theta}_0, \mathbf{h}),
\end{equation}
где 
$$
	T(\boldsymbol{\theta}, \mathbf{h}) =\boldsymbol{\theta} - \gamma \nabla L(\boldsymbol{\theta}, \mathbf{h})|_{\hat{\mathfrak{D}}}, 
$$
$\gamma$ --- длина шага градиентного спуска, $\boldsymbol{\theta}_0$ --- начальное значение параметров $\boldsymbol{\theta}$, $\hat{\mathfrak{D}}$ --- случайная подвыборка исходной выборки $\mathfrak{D}$.
\end{block}


Перепишем итоговую задачу оптимизации:
\[
	\hat{\mathbf{h}} = \argmax_{\mathbf{h} \in \mathbb{R}^h} Q( T^\eta(\boldsymbol{\theta}_0, \mathbf{h})),
\]
где $\boldsymbol{\theta}_0$ --- начальное значение параметров $\boldsymbol{\theta}$.


\end{frame}


\begin{frame}{RMAD, Maclaurin et. al, 2015 }
\begin{columns}
\begin{column}{0.5\textwidth}
\begin{enumerate}
\item Провести $\eta$ шагов оптимизации: $\boldsymbol{\theta} = T(\boldsymbol{\theta}_0, \mathbf{h})$.
\item Положим $\hat{\nabla} \mathbf{h} = \nabla_\mathbf{h} Q(\boldsymbol{\theta}, \mathbf{h}).$ 
\item Положим $d\mathbf{v} = \mathbf{0}.$
\item Для $\tau = \eta \dots 1 $ повторить:
\item $\boldsymbol{\theta}^{\tau-1} =  \boldsymbol{\theta}^{\tau} - \gamma\mathbf{v}^{\tau}.$
\item $\mathbf{v}^{\tau-1} =\mathbf{v}^{\tau} + \gamma \hat{\nabla}_{\boldsymbol{\theta}}.$
\item $d\mathbf{v} =  \gamma \hat{\nabla}_{\boldsymbol{\theta}}$.
\item $\hat{\nabla} \mathbf{h} =  \hat{\nabla} \mathbf{h} - d\mathbf{v}\nabla_{\mathbf{h}} \nabla_{\boldsymbol{\theta}} Q$.
\item $\hat{\nabla} \boldsymbol{\theta}  = \hat{\nabla} \boldsymbol{\theta}  - d\mathbf{v}\nabla_{\boldsymbol{\theta}} \nabla_{\boldsymbol{\theta}} Q$.
\end{enumerate}
\end{column}

\begin{column}{0.5\textwidth}
Алгоритм RMAD основывается на Reverse-mode differentiation.
\begin{figure}
\includegraphics[width=\textwidth]{rmd.png}
\end{figure}
\end{column}
\end{columns}
\end{frame}

\begin{frame}{DrMAD}
Алгоритм DrMad --- упрощенный RMAD. 
Вводится предположение о линейности траектории обновления параметров $\boldsymbol{\theta}$.
\begin{columns}
\begin{column}{0.5\textwidth}
\begin{enumerate}
\item Провести $\eta$ шагов оптимизации: $\boldsymbol{\theta} = T(\boldsymbol{\theta}_0, \mathbf{h})$.
\item Положим $\hat{\nabla} \mathbf{h} = \nabla_\mathbf{h} Q(\boldsymbol{\theta}, \mathbf{h}).$ 
\item Положим $d\mathbf{v} = \mathbf{0}.$
\item Для $\tau = \eta \dots 1 $ повторить:
\item $\boldsymbol{\theta}^{\tau-1} =  \boldsymbol{\theta}^{\tau} - \gamma\mathbf{v}^{\tau}.$
\item $\mathbf{v}^{\tau-1} =\mathbf{v}^{\tau} + \gamma \hat{\nabla}_{\boldsymbol{\theta}}.$
\item $d\mathbf{v} =  \gamma \hat{\nabla}_{\boldsymbol{\theta}}$.
\item $\hat{\nabla} \mathbf{h} =  \hat{\nabla} \mathbf{h} - d\mathbf{v}\nabla_{\mathbf{h}} \nabla_{\boldsymbol{\theta}} Q$.
\item $\hat{\nabla} \boldsymbol{\theta}  = \hat{\nabla} \boldsymbol{\theta}  - d\mathbf{v}\nabla_{\boldsymbol{\theta}} \nabla_{\boldsymbol{\theta}} Q$.
\end{enumerate}
\end{column}
\begin{column}{0.5\textwidth}
\begin{enumerate}
\item Провести $\eta$ шагов оптимизации: $\boldsymbol{\theta} = T(\boldsymbol{\theta}_0, \mathbf{h})$.
\item Положим $\hat{\nabla} \mathbf{h} = \nabla_\mathbf{h} Q(\boldsymbol{\theta}, \mathbf{h}).$ 
\item Положим $d\mathbf{v} = \mathbf{0}.$
\item Для $\tau = \eta \dots 1 $ повторить:
\item $\boldsymbol{\theta}^{\tau-1} = \boldsymbol{\theta}_0 + \frac{\tau-1}{\eta} \boldsymbol{\theta}^{\eta}.$
\item ~
\item $d\mathbf{v} =  \gamma \hat{\nabla}_{\boldsymbol{\theta}}$.
\item $\hat{\nabla} \mathbf{h} =  \hat{\nabla} \mathbf{h} - d\mathbf{v}\nabla_{\mathbf{h}} \nabla_{\boldsymbol{\theta}} Q$.
\item $\hat{\nabla} \boldsymbol{\theta}  = \hat{\nabla} \boldsymbol{\theta}  - d\mathbf{v}\nabla_{\boldsymbol{\theta}} \nabla_{\boldsymbol{\theta}} Q$.
\end{enumerate}
\end{column}
\end{columns}
\end{frame}


\begin{frame}{Закрытая форма оптимизации параметров}
\textbf{Утверждение (Pedregosa, 2016).}\\
Пусть $L$ --- дифференцируемая функция, такая что все стационарные точки $L$ являются локальными минимумами.
Пусть также гессиан $\mathbf{H}$ функции потерь $L$ является обратимым в каждой стационарной точке.\\
Тогда
\[
\nabla_{\mathbf{A}^{-1}}Q(T(\boldsymbol{\theta}_0), \mathbf{A}^{-1}) =  \nabla_{\mathbf{A}^{-1}}Q(\boldsymbol{\theta}^\eta, \mathbf{A}^{-1}) - \nabla_{\mathbf{A}^{-1}}\nabla_{\boldsymbol{\theta}} L(\boldsymbol{\theta}^\eta, \mathbf{A}^{-1})^\text{T}\mathbf{H}^{-1}\nabla_{\boldsymbol{\theta}}Q(\boldsymbol{\theta}^\eta, \mathbf{A}^{-1}).
\]

\textbf{Доказательство}\\
\begin{enumerate}
\item Т.к. точка $\boldsymbol{\theta}^\eta$ стационарна, то $\nabla_{\boldsymbol{\theta}} L(\boldsymbol{\theta}^\eta, \mathbf{A}^{-1}) = 0$.
\item Продифференцируем выражение по $\mathbf{A}^{-1}:$
\[
    \nabla_{\mathbf{A}^{-1}}\nabla_{\boldsymbol{\theta}} L(\boldsymbol{\theta}^\eta, \mathbf{A}^{-1} + \mathbf{H}\nabla_{\mathbf{A}^{-1}}T(\boldsymbol{\theta}_0).
\]
\item По цепному правилу:
\[
\nabla_{\mathbf{A}^{-1}}Q(T(\boldsymbol{\theta}_0), \mathbf{A}^{-1}) =  \nabla_{\mathbf{A}^{-1}}Q(\boldsymbol{\theta}^\eta, \mathbf{A}^{-1})  + \nabla_{\mathbf{A}^{-1}}T(\boldsymbol{\theta}_0)^\text{T}\nabla_{\boldsymbol{\theta}}Q(\boldsymbol{\theta}^\eta, \mathbf{A}^{-1}).
\]
\item Подставим в выражение 3 выражение 2 и получим искомое.
\end{enumerate}
\end{frame}

\begin{frame}{Жадная оптимизация гиперпараметров}
На каждом шаге оптимизации параметров $\boldsymbol{\theta}$:
\[
	\mathbf{h}' = \mathbf{h} - \gamma_{\mathbf{h}} \nabla_{\mathbf{h}}  Q \bigl(T(\boldsymbol{\theta}, \mathbf{h}) , \mathbf{h}\bigr) = \mathbf{h} - \gamma_{\mathbf{h}} \nabla_{\mathbf{h}}  Q\bigl(\boldsymbol{\theta} - \gamma \nabla L(\boldsymbol{\theta}, \mathbf{h}), \mathbf{h})\bigr),
\]
где $\gamma_{\mathbf{h}}$ --- длина шага оптимизации гиперпараметров.

\begin{itemize}
\item Можно рассматривать как упрощение алгоритма RMAD, использующее только один элемент истории обнолвения параметров.
\item Является приближением к решению закрытой формы в случае $\mathbf{H} \sim \mathbf{I}$.
\end{itemize}

\end{frame}


\begin{frame}{HOAG}
Численное приближение закрытой формы:
 \[\nabla_{\mathbf{A}^{-1}}Q(\boldsymbol{\theta}^\eta, \mathbf{A}^{-1}) - \nabla_{\mathbf{A}^{-1}}\nabla_{\boldsymbol{\theta}} L(\boldsymbol{\theta}^\eta, \mathbf{A}^{-1})^\text{T}\mathbf{H}^{-1}\nabla_{\boldsymbol{\theta}}Q(\boldsymbol{\theta}^\eta, \mathbf{A}^{-1}).\]
\begin{enumerate}
\item Провести $\eta$ шагов оптимизации: $\boldsymbol{\theta} = T(\boldsymbol{\theta}_0, \mathbf{h})$.
\item Решить линейную систему для вектора $\boldsymbol{\lambda}$: $\mathbf{H}(\boldsymbol{\theta})\boldsymbol{\lambda} =  \nabla_{\boldsymbol{\theta}} Q(\boldsymbol{\theta}, \mathbf{h})$.
\item Приближенное значение градиентов гиперпараметра вычисляется как: $\hat{\nabla}_{\mathbf{h}}Q = \nabla_{\mathbf{h}}Q(\boldsymbol{\theta}, \mathbf{h}) -\nabla_{\boldsymbol{\theta}, \mathbf{h}} L(\boldsymbol{\theta}, \mathbf{h})^T\boldsymbol{\lambda}$.
\end{enumerate}

Итоговое правило обновления:
\[
\label{eq:update_hyper}
\mathbf{h}' = \mathbf{h} - \gamma_{\mathbf{h}} \hat{\nabla}_{\mathbf{h}}Q.
\]


\end{frame}


\begin{frame}
\begin{figure}  
\includegraphics[width=0.5\textwidth]{Fig_traj.eps}
\end{figure}
\end{frame}

\begin{frame}{Эксперименты: полиномы}
\begin{figure}
  \centering
  \subfloat[Кросс-валидация]{\includegraphics[width=0.42\textwidth]{poly_cv.png}} 
 \subfloat[Evidence]{\includegraphics[width=0.42\textwidth]{poly_var.png}}
\label{fig:1}\qquad

\end{figure}
\end{frame}

\begin{frame}{Эксперименты: WISDM}
\begin{figure}  
\includegraphics[width=0.7\textwidth]{Fig_wisdm.eps}
\end{figure}
\end{frame}

\begin{frame}{Эксперименты: MNIST}
\begin{figure}  
\includegraphics[width=0.7\textwidth]{Fig_mnist.eps}
\end{figure}
\end{frame}

\begin{frame}{Эксперименты: MNIST}
Добавление гауссового шума $\mathcal{N}(\mathbf{0},\sigma^2\mathbf{I})$:
\setlength{\columnsep}{10pt}
\begin{multicols}{4}
\begin{figure}[h]
\includegraphics[width=0.10\textwidth]{./mnist0.png}
\caption*{Без шума}
\end{figure}

\begin{figure}[h]
\includegraphics[width=0.08\textwidth]{./mnist10.png}
\caption*{$\sigma=0.1$}
\end{figure}

\begin{figure}[h]
\includegraphics[width=0.08\textwidth]{./mnist25.png}
\caption*{$\sigma=0.25$}
\end{figure}

\begin{figure}[h]
\includegraphics[width=0.08\textwidth]{./mnist50.png}
\caption*{$\sigma=0.5$}
\end{figure}
\end{multicols}
\begin{center}
\includegraphics[width=0.85\textwidth]{Fig_noise.pdf}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Используемые материалы}
\begin{enumerate}
\item David J. C. MacKay, Information Theory, Inference \& Learning Algorithms, 2003
\item Christopher Bishop, Pattern Recognition and Machine Learning, 2006
\item  Dougal Maclaurin et. al, Gradient-based Hyperparameter Optimization through Reversible Learning, 2015
\item Jelena Luketina et. al, Scalable Gradient-Based Tuning of
Continuous Regularization Hyperparameters, 2016
\item Jie Fu et. al, DrMAD: Distilling Reverse-Mode Automatic Differentiation for Optimizing
Hyperparameters of Deep Neural Networks, 2016
\item Fabian Pedregosa, Hyperparameter optimization with approximate gradient, 2016
\item Bobak Shahriari et. al,  Taking the Human Out of the Loop:
A Review of Bayesian Optimization, 2016
\end{enumerate}
\end{frame}


\end{document}

